{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading libraries\n",
    "import sys, os, json, re\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading local library\n",
    "rootPath = os.path.realpath(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "libraryPath = os.path.join(rootPath, 'src', 'library')\n",
    "sys.path.append(libraryPath)\n",
    "\n",
    "import utility, k3s\n",
    "import pandas\n",
    "\n",
    "vocab = {}\n",
    "vocabSize = len(vocab) \n",
    "sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeVocab(path, vocab):\n",
    "    os.remove(path)\n",
    "    df = pandas.Series(vocab).to_frame()\n",
    "    #df = pandas.DataFrame(vocab, index=[0])\n",
    "    #print(df)\n",
    "    df.to_csv(path, mode='w', header = None)\n",
    "    \n",
    "def writeSentences(path, sentences):\n",
    "    #np.savetxt('xgboost.txt', a.values, fmt='%d', delimiter=\"\\t\", header=\"X\\tY\\tZ\\tValue\") \n",
    "    df = pandas.DataFrame(sentences)\n",
    "    #print(df)\n",
    "    df.to_csv(path, mode='w', header = None)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = os.path.join(rootPath, 'data', 'bhot', 'raw')\n",
    "charCode = {'a': '01',\n",
    "           'b': '02',\n",
    "           'c': '03',\n",
    "           'd': '04',\n",
    "           'e': '05',\n",
    "           'f': '06',\n",
    "           'g': '07',\n",
    "           'h': '08',\n",
    "           'i': '09',\n",
    "           'j': '10',\n",
    "           'k': '11',\n",
    "           'l': '12',\n",
    "           'm': '13',\n",
    "           'n': '14',\n",
    "           'o': '15',\n",
    "           'p': '16',\n",
    "           'q': '17',\n",
    "           'r': '18',\n",
    "           's': '19',\n",
    "           't': '20',\n",
    "           'u': '21',\n",
    "           'v': '22',\n",
    "           'w': '23',\n",
    "           'x': '24',\n",
    "           'y': '25',\n",
    "           'z': '26',\n",
    "           '-': '27'}\n",
    "\n",
    "def encodeStrings(strs):\n",
    "    processedStrs = {}\n",
    "    for strItem in strs:\n",
    "        strCode = ' '\n",
    "        strItem = strItem.lower()\n",
    "        \n",
    "        for char in strItem:\n",
    "            if charCode[char]:\n",
    "                strCode += charCode[char]\n",
    "            else:\n",
    "                strCode += ' '\n",
    "                \n",
    "        processedStrs[word] = strCode\n",
    "        \n",
    "    return processedStrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "'''\n",
    "\t1.\tCC\tCoordinating conjunction\n",
    "\t2.\tCD\tCardinal number\n",
    "\t3.\tDT\tDeterminer\n",
    "\t4.\tEX\tExistential there\n",
    "\t5.\tFW\tForeign word\n",
    "\t6.\tIN\tPreposition or subordinating conjunction\n",
    "\t7.\tJJ\tAdjective\n",
    "\t8.\tJJR\tAdjective, comparative\n",
    "\t9.\tJJS\tAdjective, superlative\n",
    "\t10.\tLS\tList item marker\n",
    "\t11.\tMD\tModal\n",
    "\t12.\tNN\tNoun, singular or mass\n",
    "\t13.\tNNS\tNoun, plural\n",
    "\t14.\tNNP\tProper noun, singular\n",
    "\t15.\tNNPS\tProper noun, plural\n",
    "\t16.\tPDT\tPredeterminer\n",
    "\t17.\tPOS\tPossessive ending\n",
    "\t18.\tPRP\tPersonal pronoun\n",
    "\t19.\tPRP$\tPossessive pronoun\n",
    "\t20.\tRB\tAdverb\n",
    "\t21.\tRBR\tAdverb, comparative\n",
    "\t22.\tRBS\tAdverb, superlative\n",
    "\t23.\tRP\tParticle\n",
    "\t24.\tSYM\tSymbol\n",
    "\t25.\tTO\tto\n",
    "\t26.\tUH\tInterjection\n",
    "\t27.\tVB\tVerb, base form\n",
    "\t28.\tVBD\tVerb, past tense\n",
    "\t29.\tVBG\tVerb, gerund or present participle\n",
    "\t30.\tVBN\tVerb, past participle\n",
    "\t31.\tVBP\tVerb, non-3rd person singular present\n",
    "\t32.\tVBZ\tVerb, 3rd person singular present\n",
    "\t33.\tWDT\tWh-determiner\n",
    "\t34.\tWP\tWh-pronoun\n",
    "\t35.\tWP$\tPossessive wh-pronoun\n",
    "\t36.\tWRB\tWh-adverb\n",
    "'''\n",
    "    \n",
    "def getFilteredWords(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    processedWords = []\n",
    "    #allowedPOSTypes = ['NN', 'NNP', 'NNS', 'NNPS', 'JJ', 'JJR', 'JJS' 'RB', 'RBR', 'RBS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "    allowedPOSTypes = ['NN', 'NNP', 'NNS', 'NNPS']\n",
    "    \n",
    "    lc = k3s.LC(text)\n",
    "    words = lc.getWords(text, True)\n",
    "    currentSentence = []\n",
    "    for word in words:\n",
    "        (word, type) = word\n",
    "        word = re.sub('[^a-zA-Z]+', '', word)\n",
    "        if type in ['.', '?', '!']:\n",
    "            if len(currentSentence) > 1:\n",
    "                # If more than one word than add as sentence\n",
    "                sentences.append(' '.join(currentSentence))\n",
    "            currentSentence = []\n",
    "        if len(word) < 2:\n",
    "            continue\n",
    "        if type in allowedPOSTypes:\n",
    "            #print(type + ' ' + word)\n",
    "            word = word.lower()\n",
    "            word = stemmer.stem(word)\n",
    "            processedWords.append(word)\n",
    "            if word not in currentSentence:\n",
    "                currentSentence.append(word)\n",
    "        \n",
    "    return processedWords\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildVocabulary(text):\n",
    "    global vocabSize\n",
    "    for word in text:\n",
    "        if word in vocab:\n",
    "            continue\n",
    "        vocab[word] = vocabSize\n",
    "        vocabSize += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def processTextFiles(directoryPath):\n",
    "    \n",
    "    print('Processing directory: ', directoryPath)\n",
    "    for root, dirs, files in os.walk(dataPath):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                #print('---------------------------------------')\n",
    "                file = utility.File(os.path.join(root, file))\n",
    "                text = file.read()\n",
    "                textWords = getFilteredWords(text)\n",
    "                buildVocabulary(textWords)\n",
    "                #print(len(textWords))\n",
    "                #print(vocabSize)\n",
    "                #sys.exit\n",
    "                #lc = k3s.LC(text)\n",
    "                #lc.process()\n",
    "                #contributers = lc.getContributers()\n",
    "                #result = {}\n",
    "                #result['clean_text'] = lc.getCleanText()\n",
    "                #result['most_important_featured_words'] = \",\".join(contributers[0])\n",
    "                #result['other_featured_words'] = \",\".join(contributers[1])\n",
    "                #print(encodeWords(contributers[0]))\n",
    "                #print(lc.getWords(text, True))\n",
    "                #print(result)\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory:  /notebooks/K3S/data/bhot/raw\n",
      "Files generated\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bhotPath = os.path.join(rootPath, 'data', 'bhot', 'vocab.csv')\n",
    "bhotTrainPath = os.path.join(rootPath, 'data', 'bhot', 'train.csv')\n",
    "\n",
    "processTextFiles(dataPath)\n",
    "writeVocab(bhotPath, vocab)\n",
    "writeSentences(bhotTrainPath, sentences)\n",
    "print('Files generated')\n",
    "# Reverse dictionary\n",
    "#print(dict(map(reversed, vocab.items())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordnet:  https://wordnet.princeton.edu/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
